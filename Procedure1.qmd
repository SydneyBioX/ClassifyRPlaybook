# Procedure 1

## Intro

text

## Next thing

more text

```{r message = FALSE}
library(ClassifyR)
library(sparsediscrim)
library(tidyverse)
library(SingleCellExperiment)
library(openxlsx)
library(pROC)
library(ggplot2)
library(reshape2)
```

## Simplified
```{r}
set.seed(1)
clinical = read.xlsx("clinical.xlsx")
clusters <- readRDS("cluster_result_ER_and_PR_onlyER+PR+.rds")
p <- readRDS("pseudobulk_overall_sum.rds") %>% filter(rownames(p) %in% clusters$sample_id)
p_cell <- readRDS("pseudobulk_celltype_sum.rds") %>% filter(rownames(p_cell) %in% clusters$sample_id)
outcome = clusters$cluster

# Apply crossValidate function to each dataset
classifyr_result <- lapply(data_list, function(data) {
  crossValidate(
    data,  
    outcome = classes,  
    selectionMethod = "auto", # could also use: differentMeans, limma, edgeR, bartlett, 
    classifier = "auto",      # could also use: svm, kNN
    nFeatures = 0.05 * nrow(data), # select top 5% featuers for auto selectionMethod
    nFolds = 5,              # 5-fold cross-validation, balances the groups so number of each class is balanced in each fold. Caret doesn't do this
    nRepeats = 5,            # 5 repeats
    nCores = 5               # Parallel processing with 5 cores
  )
})

performancePlot(classifyr_result)
classifyr_result <- lapply(classifyr_result, 
                           function(data) calcCVperformance(data, performanceType = "AUC"))

classifyr_result$p_cell@performance
samplesMetricMap(classifyr_result$p)
#selectionOptimisation is how to select features. (resubstitution means select from training data. Nested CV is splitting training data into training and testing)
# set to proportion rather than fixed number
```

```{r}
# classifier must exactly match these options: randomForest, DLDA, kNN, GLM, ridgeGLM, elasticNetGLM, LASSOGLM, SVM, NSC, naiveBayes, mixturesNormals, CoxPH, CoxNet, randomSurvivalForest, XGB

# Perform cross-validation for each classifier
classifyr_result <- crossValidate(
    p,  
    outcome = outcome,  
    selectionMethod = "auto", 
    classifier = c("SVM", "kNN", "randomForest", 
               "naiveBayes", "XGB"), # Use the current classifier in the loop. but result will be a list of objects so it can't be put into calcCVperformance
    nFeatures = 0.05 * nrow(p), 
    nFolds = 5,              
    nRepeats = 5,            
    nCores = 5,
    characteristicsLabel = "overall_pseudobulking"
)


classifyr_result2 <- crossValidate(
    p_cell,  
    outcome = outcome,  
    selectionMethod = "auto", 
    classifier = c("SVM", "kNN", "randomForest", 
               "naiveBayes", "XGB"), # Use the current classifier in the loop. but result will be a list of objects so it can't be put into calcCVperformance
    nFeatures = 0.05 * nrow(p_cell), 
    nFolds = 5,              
    nRepeats = 5,            
    nCores = 5,
    characteristicsLabel = "cell_specific_pseudobulking"
  )

results_both = append(classifyr_result, classifyr_result2)

results_both <- lapply(results_both, function(results) {
  calcCVperformance(results, performanceType = "Balanced Accuracy")
})

performancePlot(results_both, characteristicsList = list(x = "auto", fillColour = "characteristicsLabel")) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
```

## Overall Pseudobulked Data

```{r}
clinical = read.xlsx("clinical.xlsx")
outcome = substring(clinical$OS_STATUS,1,1)

p <- readRDS("pseudobulk_overall_sum.rds")
p_filtered <- p[, colSums(p != 0) > 0, drop = FALSE]

p_values <- sapply(colnames(p_filtered), function(feature) {
  t.test(p_filtered[[feature]] ~ outcome, na.action = na.omit)$p.value
})
selected_features <- colnames(p_filtered)[p_values < 0.05]
cat("The selected features by cell-specific pseudobulking using an fdr of 0.05 are:\n", 
    paste(selected_features, collapse = ", "), "\n")

omics_selected <- p_filtered[,selected_features]  # Subset omics data
nFeatures <- list(omics = 1:length(selected_features))

classifyr_result <- crossValidate(
  list(omics = omics_selected),  # Only use omics data
  outcome = outcome,  # Use the outcome column
  extraParams = list(prepare = list(useFeatures = list(omics = selected_features))),
  nFeatures = nFeatures,
  nFolds = 5,       # 5-fold cross-validation
  nRepeats = 5,     # 5 repeats
  nCores = 5        # Parallel processing with 5 cores
)
saveRDS(classifyr_result, file = "classifyr_result.rds")

classifyr_result_forest <- crossValidate(
  list(omics = omics_selected),  # Only use omics data
  outcome = outcome,  # Use the outcome column
  extraParams = list(prepare = list(useFeatures = list(omics = selected_features))),
  nFeatures = nFeatures,
  classifier = "randomForest",
  nFolds = 5,       # 5-fold cross-validation
  nRepeats = 5,     # 5 repeats
  nCores = 5        # Parallel processing with 5 cores
)
```

### Performance comparison between Cox proportional hazards & random survival forest

```{r}
# Extract predictions
predictions <- predictions(classifyr_result)
named_outcomes <- setNames(as.list(outcome),clinical$patientId)
# Actual outcomes
predictions$actual <- named_outcomes
predictions$actual <- unlist(predictions$actual)
# Generate the confusion matrix
confusion_matrix <- table(Predicted = predictions$class, Actual = predictions$actual)
print(confusion_matrix)
#Performance Metrics
TP <- diag(confusion_matrix)
FN <- rowSums(confusion_matrix) - TP
FP <- colSums(confusion_matrix) - TP
TN <- sum(confusion_matrix) - (TP + FP + FN)
sensitivity <- TP / (TP + FN) # Recall or Sensitivity
specificity <- TN / (TN + FP) # Specificity
precision <- TP / (TP + FP)   # Precision
accuracy <- sum(TP) / sum(confusion_matrix)
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("Metrics for predictions from overall pseudobulking:\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity (Recall) per class:", sensitivity, "\n")
cat("Specificity per class:", specificity, "\n")
cat("Precision per class:", precision, "\n")
cat("F1 Score per class:", f1_score, "\n")

#Confusion_matrix package to calculate everything: 

# Extract predictions
predictions_forest <- predictions(classifyr_result_forest)
predictions_forest$actual <- named_outcomes #actual outcome
predictions_forest$actual <- unlist(predictions_forest$actual)
# Generate the confusion matrix
confusion_matrix_forest <- table(Predicted = predictions_forest$class, Actual = predictions_forest$actual)
# View the confusion matrix
print(confusion_matrix_forest)

TP <- diag(confusion_matrix_forest)
FN <- rowSums(confusion_matrix_forest) - TP
FP <- colSums(confusion_matrix_forest) - TP
TN <- sum(confusion_matrix_forest) - (TP + FP + FN)
# Performance Metrics
sensitivity <- TP / (TP + FN) # Recall or Sensitivity
specificity <- TN / (TN + FP) # Specificity
precision <- TP / (TP + FP)   # Precision
accuracy <- sum(TP) / sum(confusion_matrix_forest)
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

cat("Metrics for predictions from overall pseudobulking using random forest:\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity (Recall) per class:", sensitivity, "\n")
cat("Specificity per class:", specificity, "\n")
cat("Precision per class:", precision, "\n")
cat("F1 Score per class:", f1_score, "\n")
```

## Cell-specific Pseudobulked Data

```{r}
p_cell <- readRDS("pseudobulk_celltype_sum.rds")
p_cell_filtered <- p_cell[, colSums(p_cell != 0) > 0, drop = FALSE]

p_values_cell <- sapply(colnames(p_cell_filtered), function(feature) {
  t.test(p_cell_filtered[[feature]] ~ outcome, na.action = na.omit)$p.value
})

selected_cell_features <- colnames(p_cell_filtered)[p_values_cell < 0.05]
cat("The selected features by cell-specific pseudobulking using an fdr of 0.05 are:\n", 
    paste(selected_cell_features, collapse = ", "), "\n")

omics_selected_cell <- p_cell_filtered[,selected_cell_features]  # Subset omics data
nFeatures_cell <- list(omics = 1:length(selected_cell_features))

classifyr_result_cell <- crossValidate(
  list(omics = omics_selected_cell),  
  outcome = outcome,  # Use the outcome column
  extraParams = list(prepare = list(useFeatures = list(omics = selected_cell_features))),
  nFeatures = nFeatures_cell,
  nFolds = 5,       # 5-fold cross-validation
  nRepeats = 5,     # 5 repeats
  nCores = 5        # Parallel processing with 5 cores
)
saveRDS(classifyr_result_cell, file = "classifyr_result_cell.rds")

classifyr_result_cell_forest <- crossValidate(
  list(omics = omics_selected_cell),  # Only use omics data
  outcome = outcome,  # Use the outcome column
  extraParams = list(prepare = list(useFeatures = list(omics = selected_cell_features))),
  nFeatures = nFeatures_cell,
  classifier = "randomForest",
  nFolds = 5,       # 5-fold cross-validation
  nRepeats = 5,     # 5 repeats
  nCores = 5        # Parallel processing with 5 cores
)
```

### Performance comparison between Cox proportional hazards & random survival forest

```{r}
# Extract predictions
predictions_cell <- predictions(classifyr_result_cell)
named_outcomes <- setNames(as.list(outcome),clinical$patientId)
# Actual Outcomes
predictions_cell$actual <- named_outcomes
predictions_cell$actual <- unlist(predictions_cell$actual)
# Generate the confusion matrix
confusion_matrix_cell <- table(Predicted = predictions_cell$class, Actual = predictions_cell$actual)
print(confusion_matrix_cell)

TP <- diag(confusion_matrix_cell)
FN <- rowSums(confusion_matrix_cell) - TP
FP <- colSums(confusion_matrix_cell) - TP
TN <- sum(confusion_matrix_cell) - (TP + FP + FN)
# Performance Metrics
sensitivity <- TP / (TP + FN) # Recall or Sensitivity
specificity <- TN / (TN + FP) # Specificity
precision <- TP / (TP + FP)   # Precision'accuracy <- sum(TP) / sum(confusion_matrix_cell_forest)
accuracy <- sum(TP) / sum(confusion_matrix_cell)
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("Metrics for predictions from cell-specific pseudobulking:\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity (Recall) per class:", sensitivity, "\n")
cat("Specificity per class:", specificity, "\n")
cat("Precision per class:", precision, "\n")
cat("F1 Score per class:", f1_score, "\n")


# Extract predictions
predictions_cell_forest <- predictions(classifyr_result_cell_forest)
predictions_cell_forest$actual <- named_outcomes #actual outcome
predictions_cell_forest$actual <- unlist(predictions_cell_forest$actual)
# Generate the confusion matrix
confusion_matrix_cell_forest <- table(Predicted = predictions_cell_forest$class, Actual = predictions_cell_forest$actual)
# View the confusion matrix
print(confusion_matrix_cell_forest)

# Total True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)
TP <- diag(confusion_matrix_cell_forest)
FN <- rowSums(confusion_matrix_cell_forest) - TP
FP <- colSums(confusion_matrix_cell_forest) - TP
TN <- sum(confusion_matrix_cell_forest) - (TP + FP + FN)

# Calculating metrics for each class
sensitivity <- TP / (TP + FN) # Recall or Sensitivity
specificity <- TN / (TN + FP) # Specificity
precision <- TP / (TP + FP)   # Precision
accuracy <- sum(TP) / sum(confusion_matrix_cell_forest)
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

cat("Metrics for predictions from cell-specific pseudobulking using random forest:\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity (Recall) per class:", sensitivity, "\n")
cat("Specificity per class:", specificity, "\n")
cat("Precision per class:", precision, "\n")
cat("F1 Score per class:", f1_score, "\n")
```

## Plots
```{r}
# Function to generate ROC curve
generate_roc <- function(predictions, outcome) {
  actual <- as.numeric(as.factor(predictions$outcome))
  # Use predicted probabilities
  pred_prob <- predictions$X0
  # Compute ROC
  roc_obj <- roc(actual, pred_prob)
  return(roc_obj)
}

predictions = as.data.frame(predictions)
# Generate ROC curves for both methods
roc_overall <- generate_roc(predictions, actual)
roc_overall_forest <- generate_roc(predictions_forest, outcome)
roc_cell <- generate_roc(predictions_cell, outcome)
roc_cell_forest <- generate_roc(predictions_cell_forest, outcome)

# Plot ROC curves
roc_plot <- ggplot() +
  geom_line(data = data.frame(fpr = 1 - roc_overall$specificities, tpr = roc_overall$sensitivities), aes(x = fpr, y = tpr), color = "blue", linetype = "solid", size = 1) +
  geom_line(data = data.frame(fpr = 1 - roc_overall_forest$specificities, tpr = roc_overall_forest$sensitivities), aes(x = fpr, y = tpr), color = "red", linetype = "dashed", size = 1) +
  geom_line(data = data.frame(fpr = 1 - roc_cell$specificities, tpr = roc_cell$sensitivities), aes(x = fpr, y = tpr), color = "green", linetype = "solid", size = 1) +
  geom_line(data = data.frame(fpr = 1 - roc_cell_forest$specificities, tpr = roc_cell_forest$sensitivities), aes(x = fpr, y = tpr), color = "purple", linetype = "dashed", size = 1) +
  labs(title = "ROC Curves", x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)") +
  theme_minimal() +
  scale_color_manual(name = "Model", 
                     values = c("blue", "red", "green", "purple"),
                     labels = c("Overall (Cox)", "Overall (Random Forest)", "Cell-specific (Cox)", "Cell-specific (Random Forest)")) +
  theme(legend.position = "bottom") +
  coord_equal()

print(roc_plot)

# Add AUC values
auc_overall <- auc(roc_overall)
auc_overall_forest <- auc(roc_overall_forest)
auc_cell <- auc(roc_cell)
auc_cell_forest <- auc(roc_cell_forest)

cat("AUC for overall (Cox):", auc_overall, "\n")
cat("AUC for overall (Random Forest):", auc_overall_forest, "\n")
cat("AUC for cell-specific (Cox):", auc_cell, "\n")
cat("AUC for cell-specific (Random Forest):", auc_cell_forest, "\n")
```

